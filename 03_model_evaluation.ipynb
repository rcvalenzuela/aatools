{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model_evaluation\n",
    "\n",
    "> The model evaluation module provides a set of functions for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def metrics_by_threshold(y_true, # Ground truth (correct) target values\n",
    "                         y_proba, # Estimated probability as returned by a binary classifier\n",
    "                         n_points:int=100): # Number of points on which to evaluate\n",
    "    \"\"\"Returns dataframe with several binary classification metrics as a function of the decision threshold\"\"\"\n",
    "    \n",
    "    # Calculate tn, fp, fn, tp for different thresholds\n",
    "    cm_thr = []\n",
    "    for thr in np.linspace(0,1,100):\n",
    "        y_pred = np.where(y_proba >= thr, 1, 0)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        cm_thr.append([thr, tn, fp, fn, tp])\n",
    "   \n",
    "    cm_thr_df = pd.DataFrame(cm_thr, columns=['thr', 'tn', 'fp', 'fn', 'tp'])\n",
    "   \n",
    "    ## Calculation of metrics\n",
    "\n",
    "    cm_thr_df['precision'] = cm_thr_df['tp'] / (cm_thr_df['tp'] + cm_thr_df['fp'])\n",
    "    cm_thr_df['recall'] = cm_thr_df['tp'] / (cm_thr_df['tp'] + cm_thr_df['fn'])\n",
    "    cm_thr_df['accuracy'] = (cm_thr_df['tp'] + cm_thr_df['tn']) / (cm_thr_df['tp'] + cm_thr_df['fn'] + cm_thr_df['fp'] + cm_thr_df['tn'])\n",
    "    cm_thr_df['f1'] = (2*cm_thr_df['tp']) / (2*cm_thr_df['tp'] + cm_thr_df['fn'] + cm_thr_df['fp'])\n",
    "    cm_thr_df['specificity'] = cm_thr_df['tn'] / (cm_thr_df['tn'] + cm_thr_df['fp'])\n",
    "   \n",
    "    return cm_thr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mthr = metrics_by_threshold(ea_df['solicitud_baja'], ea_df['y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(mthr['thr'], mthr['f1'], color='b', label=f'F1-score')\n",
    "# ax.plot(mthr['thr'], mthr['precision'], color='g', label=f'Precision')\n",
    "# ax.plot(mthr['thr'], mthr['recall'], color='m', label=f'Recall')\n",
    "# ax.plot(mthr['thr'], mthr['specificity'], color='k', label=f'Specificity')\n",
    "# ax.legend()\n",
    "# ax.grid()\n",
    "# ax.set_xlabel('Threshold')\n",
    "# ax.set_ylabel('Metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
