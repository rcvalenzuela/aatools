{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: The model evaluation module provides a set of functions for model evaluation\n",
    "output-file: model_evaluation.html\n",
    "title: model_evaluation\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/rcvalenzuela/aatools/blob/master/aatools/model_evaluation.py#L13){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### metrics_by_threshold\n",
       "\n",
       ">      metrics_by_threshold (y_true, y_proba, n_points:int=100)\n",
       "\n",
       "Returns dataframe with several binary classification metrics as a function of the decision threshold\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| y_true |  |  | Ground truth (correct) target values |\n",
       "| y_proba |  |  | Estimated probability as returned by a binary classifier |\n",
       "| n_points | int | 100 | Number of points on which to evaluate |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/rcvalenzuela/aatools/blob/master/aatools/model_evaluation.py#L13){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### metrics_by_threshold\n",
       "\n",
       ">      metrics_by_threshold (y_true, y_proba, n_points:int=100)\n",
       "\n",
       "Returns dataframe with several binary classification metrics as a function of the decision threshold\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| y_true |  |  | Ground truth (correct) target values |\n",
       "| y_proba |  |  | Estimated probability as returned by a binary classifier |\n",
       "| n_points | int | 100 | Number of points on which to evaluate |"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(metrics_by_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "#mthr = metrics_by_threshold(ea_df['solicitud_baja'], ea_df['y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(mthr['thr'], mthr['f1'], color='b', label=f'F1-score')\n",
    "# ax.plot(mthr['thr'], mthr['precision'], color='g', label=f'Precision')\n",
    "# ax.plot(mthr['thr'], mthr['recall'], color='m', label=f'Recall')\n",
    "# ax.plot(mthr['thr'], mthr['specificity'], color='k', label=f'Specificity')\n",
    "# ax.legend()\n",
    "# ax.grid()\n",
    "# ax.set_xlabel('Threshold')\n",
    "# ax.set_ylabel('Metric')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
